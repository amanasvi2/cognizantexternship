{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7d2897",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
    "\n",
    "## Instructions\n",
    "1. Change the prompt below to experiment with different types of text generation.\n",
    "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
    "3. Generate at least 3 samples with different prompts and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The benefits of exercise include being physically fit and having a stable home environment.\n",
      "\n",
      "The first question that arises after a workout\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Set your prompt\n",
    "prompt = 'The benefits of exercise include'\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=25, temperature=0.9)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69a033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The impact of AI on the future of work is clear. Many experts think that AI will replace jobs, for example, by more automated methods of work, like data-driven and analytical work. But there's still much we don't know about how\n",
      "Once upon a time, there was a kingdom of the gods, and there were no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and no gods but your gods, and\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different prompts\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=50, temperature=0.8)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "prompt = 'Once upon a time, there was a kingdom'\n",
    "result = generator(prompt, max_length=100, temperature=0.6)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0d32",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Now that you have experimented with text generation, write a brief report on your observations.\n",
    "\n",
    "1. What patterns did you notice in the generated text?\n",
    "2. How did changing the temperature affect the creativity and coherence of the text?\n",
    "3. What types of prompts yielded the most coherent results?\n",
    "4. What are the limitations of GPT-2 based on your experimentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4896d45",
   "metadata": {},
   "source": [
    "1) After experimenting with GPT-2 for text generation, several patterns and behaviors emerged in the generated outputs. One noticeable pattern was that the model tended to produce grammatically correct sentences, especially for shorter generations. It was also clear that GPT-2 relies on common phrases, sentence structures, and associations it has learned from its training data. The text often followed logical progression when given a clear prompt, and it was capable of copying various writing styles depending on the input.\n",
    "\n",
    "2) Temperature played a role in shaping the model’s output. At lower temperature values (0.7), the text was more repetitive and predictable, but also more coherent. At higher temperatures (1.0 or above), the text became more diverse, sometimes at the expense of logical consistency. This showed the trade-off between creativity and coherence in language generation tasks.\n",
    "\n",
    "3) Prompts that were specific and grammatically complete produced the most coherent responses. For example, prompts that copied article openings, questions, or storytelling formats helped guide the model and led to more structured results. In contrast, vague prompts tended to yield disjointed completions.\n",
    "\n",
    "4) Despite its capabilities, GPT-2 has several limitations. The model sometimes “hallucinates” facts, making up details that are sound true but false. It also lacks true understanding of context, meaning it can produce biased content without awareness. Additionally, it can struggle with long-term coherence in extended generations and does not retain information between sessions. These limitations show the importance of careful prompt design and human oversight when using GPT-2 in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
